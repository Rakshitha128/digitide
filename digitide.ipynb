{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4055debe-7a5b-4afe-a3f4-91f8fbfab6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   InvoiceNo StockCode                          Description  Quantity  \\\n",
      "0     536365    85123A   white hanging heart t-light holder         6   \n",
      "1     536365     71053                  white metal lantern         6   \n",
      "2     536365    84406B       cream cupid hearts coat hanger         8   \n",
      "3     536365    84029G  knitted union flag hot water bottle         6   \n",
      "4     536365    84029E       red woolly hottie white heart.         6   \n",
      "5     536365     22752         set 7 babushka nesting boxes         2   \n",
      "6     536365     21730    glass star frosted t-light holder         6   \n",
      "7     536366     22633               hand warmer union jack         6   \n",
      "8     536366     22632            hand warmer red polka dot         6   \n",
      "9     536367     84879        assorted colour bird ornament        32   \n",
      "10    536367     22745            poppy's playhouse bedroom         6   \n",
      "11    536367     22748            poppy's playhouse kitchen         6   \n",
      "12    536367     22749    feltcraft princess charlotte doll         8   \n",
      "13    536367     22310               ivory knitted mug cosy         6   \n",
      "14    536367     84969   box of 6 assorted colour teaspoons         6   \n",
      "15    536367     22623         box of vintage jigsaw blocks         3   \n",
      "16    536367     22622       box of vintage alphabet blocks         2   \n",
      "17    536367     21754             home building block word         3   \n",
      "18    536367     21755             love building block word         3   \n",
      "19    536367     21777          recipe box with metal heart         4   \n",
      "\n",
      "       InvoiceDate  UnitPrice  CustomerID         Country  \n",
      "0   12/1/2010 8:26       2.55     17850.0  United Kingdom  \n",
      "1   12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
      "2   12/1/2010 8:26       2.75     17850.0  United Kingdom  \n",
      "3   12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
      "4   12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
      "5   12/1/2010 8:26       7.65     17850.0  United Kingdom  \n",
      "6   12/1/2010 8:26       4.25     17850.0  United Kingdom  \n",
      "7   12/1/2010 8:28       1.85     17850.0  United Kingdom  \n",
      "8   12/1/2010 8:28       1.85     17850.0  United Kingdom  \n",
      "9   12/1/2010 8:34       1.69     13047.0  United Kingdom  \n",
      "10  12/1/2010 8:34       2.10     13047.0  United Kingdom  \n",
      "11  12/1/2010 8:34       2.10     13047.0  United Kingdom  \n",
      "12  12/1/2010 8:34       3.75     13047.0  United Kingdom  \n",
      "13  12/1/2010 8:34       1.65     13047.0  United Kingdom  \n",
      "14  12/1/2010 8:34       4.25     13047.0  United Kingdom  \n",
      "15  12/1/2010 8:34       4.95     13047.0  United Kingdom  \n",
      "16  12/1/2010 8:34       9.95     13047.0  United Kingdom  \n",
      "17  12/1/2010 8:34       5.95     13047.0  United Kingdom  \n",
      "18  12/1/2010 8:34       5.95     13047.0  United Kingdom  \n",
      "19  12/1/2010 8:34       7.95     13047.0  United Kingdom  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data.csv\", encoding=\"utf-8\", encoding_errors=\"ignore\")\n",
    "\n",
    "df['UnitPrice'] = df['UnitPrice'].fillna(df['UnitPrice'].mean())\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df['Description'] = df['Description'].str.lower().str.strip()\n",
    "\n",
    "print(df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa4847d1-f0b0-4acc-b7aa-9427a1cd03d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      movieId                                            title  \\\n",
      "2336     3096                            My Man Godfrey (1957)   \n",
      "7383    79897                                   Get Low (2009)   \n",
      "7157    72142           Love Exposure (Ai No Mukidashi) (2008)   \n",
      "7117    71268  Tyler Perry's I Can Do Bad All by Myself (2009)   \n",
      "7030    69211                             Boy Eats Girl (2005)   \n",
      "\n",
      "                           genres  rating  \n",
      "2336                       Comedy     5.0  \n",
      "7383         Comedy|Drama|Mystery     5.0  \n",
      "7157  Action|Comedy|Drama|Romance     5.0  \n",
      "7117                 Comedy|Drama     5.0  \n",
      "7030                Comedy|Horror     5.0  \n"
     ]
    }
   ],
   "source": [
    "#4.\tMovie Recommendation (Basic)\n",
    "import pandas as pd\n",
    "\n",
    "movies = pd.read_csv(\"movies.csv\")\n",
    "ratings = pd.read_csv(\"ratings.csv\")\n",
    "\n",
    "avg_ratings = ratings.groupby(\"movieId\")[\"rating\"].mean()\n",
    "\n",
    "movies = movies.merge(avg_ratings, on=\"movieId\")\n",
    "\n",
    "top_comedy = movies[movies[\"genres\"].str.contains(\"Comedy\", na=False)] \\\n",
    "                .sort_values(\"rating\", ascending=False) \\\n",
    "                .head(5)\n",
    "\n",
    "print(top_comedy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "411d2971-0cac-455b-860e-6b0671c88655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8403854617999571\n"
     ]
    }
   ],
   "source": [
    "#1.\tLoan Default Prediction (Classification)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "df = pd.read_csv(\"accepted_2007_to_2018Q4.csv\",low_memory=False)\n",
    "\n",
    "df = df[[\"loan_amnt\", \"int_rate\", \"annual_inc\", \"loan_status\"]].dropna()\n",
    "\n",
    "df[\"default\"] = df[\"loan_status\"].apply(lambda x: 1 if \"Default\" in str(x) or \"Charged Off\" in str(x) else 0)\n",
    "\n",
    "X = df[[\"loan_amnt\", \"int_rate\", \"annual_inc\"]]\n",
    "y = df[\"default\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy:\", model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eb02d36-52c7-47e7-a6ef-8d4bf8309ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)  \\\n",
      "0           1    Male   19                  15                      39   \n",
      "1           2    Male   21                  15                      81   \n",
      "2           3  Female   20                  16                       6   \n",
      "3           4  Female   23                  16                      77   \n",
      "4           5  Female   31                  17                      40   \n",
      "\n",
      "   Cluster  \n",
      "0        2  \n",
      "1        2  \n",
      "2        2  \n",
      "3        2  \n",
      "4        2  \n"
     ]
    }
   ],
   "source": [
    "#3.\tCustomer Segmentation (Clustering)\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df = pd.read_csv(\"Mall_Customers.csv\")\n",
    "\n",
    "X = df[[\"Annual Income (k$)\", \"Spending Score (1-100)\"]]\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "df[\"Cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0eb096e5-50e3-4fb4-803e-6730f13f0029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9995611109160493\n"
     ]
    }
   ],
   "source": [
    "#1.\tRandom Forest for Fraud Detection\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "X = df.drop(\"Class\", axis=1)\n",
    "y = df[\"Class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy:\", model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f6ce297-6bd9-4d22-821a-124976b962fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7467532467532467\n"
     ]
    }
   ],
   "source": [
    "#4.\tXGBoost for Diabetes Prediction\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "\n",
    "X = df.drop(\"Outcome\", axis=1)\n",
    "y = df[\"Outcome\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "241d7060-be86-4f6c-b530-07c5f044d861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training ANN model...\n",
      "Epoch 1/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - accuracy: 0.9257 - loss: 0.2560 - val_accuracy: 0.9678 - val_loss: 0.1120\n",
      "Epoch 2/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.9673 - loss: 0.1087 - val_accuracy: 0.9747 - val_loss: 0.0879\n",
      "Epoch 3/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.9766 - loss: 0.0757 - val_accuracy: 0.9760 - val_loss: 0.0771\n",
      "Epoch 4/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 9ms/step - accuracy: 0.9818 - loss: 0.0570 - val_accuracy: 0.9805 - val_loss: 0.0662\n",
      "Epoch 5/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.9859 - loss: 0.0442 - val_accuracy: 0.9808 - val_loss: 0.0658\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9785 - loss: 0.0764\n",
      "\n",
      "ANN Test Accuracy: 0.9785\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"\\nTraining ANN model...\")\n",
    "model.fit(X_train, y_train, epochs=5, validation_split=0.1)\n",
    "\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nANN Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aba1b08-7192-473d-9437-83d49332aa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 115ms/step - accuracy: 0.6827 - loss: 0.5818 - val_accuracy: 0.6222 - val_loss: 0.6551\n",
      "Epoch 2/3\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 115ms/step - accuracy: 0.7982 - loss: 0.4421 - val_accuracy: 0.7806 - val_loss: 0.5027\n",
      "Epoch 3/3\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 111ms/step - accuracy: 0.8900 - loss: 0.2737 - val_accuracy: 0.7700 - val_loss: 0.5405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1fe941328b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)\n",
    "\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=200)\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=200)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(10000, 128),\n",
    "    layers.SimpleRNN(64),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=3, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ede47a-e313-41f2-89f7-42f59616524f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
